{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Branch Model Training\n",
    "Training notebook for the DualBranchModel (Summary + VPIN LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.utils.rnn as rnn_utils\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\n# Import CTAFlow modules\nfrom CTAFlow.models.deep_learning.multi_branch.dual_model import DualBranchModel\nfrom CTAFlow.models.intraday_momentum import DeepIDMomentum\nfrom CTAFlow.data.model_datasets import DualDataset\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATA_DIR = Path('/content/drive/MyDrive/cl')\n\n# File paths - adjust these to match your files\nINTRADAY_PATH = DATA_DIR / 'cl_intraday.csv'  # Intraday OHLCV data (required for DeepIDMomentum)\nCL_FEATURES_PATH = DATA_DIR / 'cl_features.csv'  # Summary features\nVPIN_PATH = DATA_DIR / 'CL_vpin.parquet'  # Sequential data (VPIN)\nTARGET_PATH = DATA_DIR / 'targets.csv'  # Optional, set to None if targets in cl_features\n\n# Model hyperparameters\nMAX_SEQ_LEN = 200\nLSTM_HIDDEN = 64\nDENSE_HIDDEN = 128\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-3\nEPOCHS = 50\nVAL_SPLIT = 0.2\n\n# Normalization configuration\nNORMALIZE_SEQUENTIAL = True  # Whether to normalize sequential features using DeepIDMomentum\nSEQUENTIAL_COLS = ['vpin', 'bucket_return', 'log_duration']  # Columns to use from sequential data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Loading & Normalization\n\nWe use `DeepIDMomentum` to load and normalize the sequential data, then create a `DualDataset` for training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data using DeepIDMomentum for normalization\nprint(\"Loading data with DeepIDMomentum...\")\n\n# Initialize DeepIDMomentum to leverage its normalization capabilities\ndeep_model = DeepIDMomentum.from_files(\n    intraday_path=str(INTRADAY_PATH),\n    features_path=str(CL_FEATURES_PATH),\n    sequential_path=str(VPIN_PATH),\n    target_path=str(TARGET_PATH) if TARGET_PATH and TARGET_PATH.exists() else None,\n    target_col='Target'\n)\n\n# Normalize sequential features if enabled\nif NORMALIZE_SEQUENTIAL:\n    print(\"Normalizing sequential features...\")\n    # Get sequential columns that exist in the data\n    available_cols = [col for col in SEQUENTIAL_COLS if col in deep_model.sequential_data.columns]\n    \n    # DeepIDMomentum's normalize_sequential_features handles market profile data\n    # For VPIN data, we'll apply standard scaling\n    from sklearn.preprocessing import StandardScaler\n    \n    scaler = StandardScaler()\n    deep_model.sequential_data[available_cols] = scaler.fit_transform(\n        deep_model.sequential_data[available_cols]\n    )\n    deep_model.training_data['sequential'] = deep_model.sequential_data\n    \n    print(f\"Normalized columns: {available_cols}\")\n\nprint(\"DeepIDMomentum data loaded and normalized successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DualDataset from DeepIDMomentum data\nprint(\"Creating DualDataset...\")\n\n# Extract the normalized data from DeepIDMomentum\nsummary_data = deep_model.training_data['summary']\nsequential_data = deep_model.training_data['sequential']\ntarget_data = deep_model.target_data\n\n# Create DualDataset\ndataset = DualDataset(\n    summary_data=summary_data,\n    sequential_data=sequential_data,\n    target_data=target_data,\n    max_len=MAX_SEQ_LEN,\n    sequential_cols=SEQUENTIAL_COLS,\n    target_col='Target' if 'Target' in summary_data.columns else None,\n    date_col='Datetime' if 'Datetime' in summary_data.columns else None\n)\n\nprint(f\"Summary features: {len(dataset.feature_cols)}\")\nprint(f\"Sequential features: {dataset.n_sequential_features}\")\nprint(f\"Total samples: {len(dataset)}\")\nprint(f\"Feature columns: {dataset.feature_cols}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Collate Function\n\nCustom collate function for DualDataset to handle variable-length sequences."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle variable-length sequences from DualDataset.\n    Pads sequences to the max length in the batch.\n    \"\"\"\n    summaries, sequential_seqs, targets, lengths = zip(*batch)\n    \n    # Stack summaries and targets\n    summaries = torch.stack(summaries)\n    targets = torch.stack(targets)\n    lengths = torch.tensor(lengths)\n    \n    # Pad sequential sequences\n    sequential_padded = rnn_utils.pad_sequence(sequential_seqs, batch_first=True, padding_value=0.0)\n    \n    return summaries, sequential_padded, targets, lengths"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train/Validation Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train/Val split (time-based)\nn_samples = len(dataset)\nn_val = int(n_samples * VAL_SPLIT)\nn_train = n_samples - n_val\n\n# Use sequential split for time series\ntrain_indices = list(range(n_train))\nval_indices = list(range(n_train, n_samples))\n\ntrain_dataset = torch.utils.data.Subset(dataset, train_indices)\nval_dataset = torch.utils.data.Subset(dataset, val_indices)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Create DataLoaders"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DataLoaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=2\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test batch\nsummary, sequential, target, lengths = next(iter(train_loader))\nprint(f\"Summary shape: {summary.shape}\")\nprint(f\"Sequential shape: {sequential.shape}\")\nprint(f\"Target shape: {target.shape}\")\nprint(f\"Lengths (first 5): {lengths[:5]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize DualBranchModel (imported from CTAFlow)\nmodel = DualBranchModel(\n    summary_input_dim=len(dataset.feature_cols),\n    vpin_input_dim=dataset.n_sequential_features,\n    lstm_hidden_dim=LSTM_HIDDEN,\n    dense_hidden_dim=DENSE_HIDDEN\n).to(device)\n\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(\"\\nModel Architecture:\")\nprint(f\"  Summary Branch: {len(dataset.feature_cols)} -> {DENSE_HIDDEN} -> {DENSE_HIDDEN//2}\")\nprint(f\"  Sequential Branch: {dataset.n_sequential_features} -> LSTM({LSTM_HIDDEN})\")\nprint(f\"  Fusion Head: {DENSE_HIDDEN//2 + LSTM_HIDDEN} -> 64 -> 1\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for summary, sequential, target, lengths in loader:\n        summary = summary.to(device)\n        sequential = sequential.to(device)\n        target = target.to(device).unsqueeze(1)\n        lengths = lengths.to(device)\n        \n        optimizer.zero_grad()\n        output = model(summary, sequential, lengths)\n        loss = criterion(output, target)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for summary, sequential, target, lengths in loader:\n            summary = summary.to(device)\n            sequential = sequential.to(device)\n            target = target.to(device).unsqueeze(1)\n            lengths = lengths.to(device)\n            \n            output = model(summary, sequential, lengths)\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            \n            all_preds.extend(output.cpu().numpy().flatten())\n            all_targets.extend(target.cpu().numpy().flatten())\n    \n    return total_loss / len(loader), np.array(all_preds), np.array(all_targets)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Training'):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_preds, val_targets = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), DATA_DIR / 'best_model.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        corr = np.corrcoef(val_preds, val_targets)[0, 1]\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Corr: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train')\n",
    "axes[0].plot(val_losses, label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "\n",
    "# Load best model and get final predictions\n",
    "model.load_state_dict(torch.load(DATA_DIR / 'best_model.pt'))\n",
    "_, val_preds, val_targets = validate(model, val_loader, criterion, device)\n",
    "\n",
    "axes[1].scatter(val_targets, val_preds, alpha=0.5, s=10)\n",
    "axes[1].plot([val_targets.min(), val_targets.max()], [val_targets.min(), val_targets.max()], 'r--')\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title(f'Predictions (Corr: {np.corrcoef(val_preds, val_targets)[0,1]:.4f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'training_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"=== Final Validation Metrics ===\")\n",
    "print(f\"MSE: {mean_squared_error(val_targets, val_preds):.6f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(val_targets, val_preds)):.6f}\")\n",
    "print(f\"MAE: {mean_absolute_error(val_targets, val_preds):.6f}\")\n",
    "print(f\"R2: {r2_score(val_targets, val_preds):.6f}\")\n",
    "print(f\"Correlation: {np.corrcoef(val_preds, val_targets)[0,1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'config': {\n",
    "        'summary_input_dim': len(dataset.feature_cols),\n",
    "        'vpin_input_dim': dataset.n_vpin_features,\n",
    "        'lstm_hidden_dim': LSTM_HIDDEN,\n",
    "        'dense_hidden_dim': DENSE_HIDDEN,\n",
    "    }\n",
    "}, DATA_DIR / 'dual_branch_checkpoint.pt')\n",
    "\n",
    "print(f\"Model saved to {DATA_DIR / 'dual_branch_checkpoint.pt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}